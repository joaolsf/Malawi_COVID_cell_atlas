{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f642f7db",
   "metadata": {},
   "source": [
    "# IMC-Denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa81dad",
   "metadata": {},
   "source": [
    "# Default setup IMC-Denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92e103",
   "metadata": {},
   "source": [
    "1- Follow the instructions under the 'Installation' header from here: https://github.com/PENGLU-WashU/IMC_Denoise. In brief, you need to setup a new conda environment and install some packages with specific version numbers, and then clone and install the IMCDenoise package from Github.\n",
    "2- Run the following command in Anaconda prompt to install a couple of extra packages we will need in the new environment: conda install tqdm pandas seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2569c8d",
   "metadata": {},
   "source": [
    "# Denoising from non-Bodenmiller pipeline sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4765299f",
   "metadata": {},
   "source": [
    "This will also work with images from a non-Bodenmiller source. When you define channels, it will search through the source directories for any images matching that name. Therefore, be careful with channels with overlapping names! Eg. CD4 and CD45!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477481c",
   "metadata": {},
   "source": [
    "# Imports and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "594d5b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, abspath, exists\n",
    "from glob import glob\n",
    "import tifffile as tp\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "from pathlib import Path\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import zscore\n",
    "import distutils.dir_util\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df3450d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b482e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorflow.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d93017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dd88df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-12.6-arm64-arm-64bit\n",
      "Tensor Flow Version: 2.10.0\n",
      "Keras Version: 2.10.0\n",
      "\n",
      "Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:33) \n",
      "[Clang 13.0.1 ]\n",
      "Pandas 1.5.0\n",
      "Scikit-Learn 1.1.2\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python Platform: {platform.platform()}\")\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {tensorflow.keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e84dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IMC_Denoise.IMC_Denoise_main.DIMR import DIMR\n",
    "from IMC_Denoise.IMC_Denoise_main.DeepSNF import DeepSNF\n",
    "from IMC_Denoise.DeepSNF_utils.DeepSNF_DataGenerator import DeepSNF_DataGenerator, load_training_patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0f9aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "accb3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "### These are adapted from functions from IMC_Denoise\n",
    "\n",
    "def load_single_img(filename):\n",
    "    \n",
    "    \"\"\"\n",
    "    Loading single image from directory.\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : The image file name, must end with .tiff.\n",
    "        DESCRIPTION.\n",
    "    Returns\n",
    "    -------\n",
    "    Img_in : int or float\n",
    "        Loaded image data.\n",
    "    \"\"\"\n",
    "    if filename.endswith('.tiff') or filename.endswith('.tif'):\n",
    "        Img_in = tp.imread(filename).astype('float32')\n",
    "    else:\n",
    "        raise ValueError('Raw file should end with tiff or tif!')\n",
    "    if Img_in.ndim != 2:\n",
    "        raise ValueError('Single image should be 2d!')\n",
    "    return Img_in\n",
    "\n",
    "def load_imgs_from_directory(load_directory,channel_name,quiet=False):\n",
    "    Img_collect = []\n",
    "    img_folders = glob(join(load_directory, \"*\", \"\"))\n",
    "    Img_file_list=[]\n",
    "\n",
    "    if not quiet:\n",
    "        print('Image data loaded from ...\\n')\n",
    "    \n",
    "    for sub_img_folder in img_folders:\n",
    "        Img_list = [f for f in listdir(sub_img_folder) if isfile(join(sub_img_folder, f)) & (f.endswith(\".tiff\") or f.endswith(\".tif\"))]\n",
    "        for Img_file in Img_list:\n",
    "            if channel_name.lower() in Img_file.lower():\n",
    "                Img_read = load_single_img(sub_img_folder + Img_file)\n",
    "                \n",
    "                if not quiet:\n",
    "                    print(sub_img_folder + Img_file)\n",
    "                \n",
    "                Img_file_list.append(Img_file)\n",
    "                Img_collect.append(Img_read)\n",
    "                break\n",
    "\n",
    "    if not quiet:\n",
    "        print('\\n' + 'Image data loaded completed!')\n",
    "    \n",
    "    if not Img_collect:\n",
    "        print(f'No such channel as {channel_name}. Please check the channel name again!')\n",
    "        return\n",
    "\n",
    "    return Img_collect, Img_file_list, img_folders\n",
    "\n",
    "\n",
    "\n",
    "# This function takes the stacked 'ometiffs' which the Bodenmiller pipeline extracts from the MCD files right at the start of the pipeline, then extracts them to individual channels with sensible names\n",
    "\n",
    "def unstack_tiffs(input_folder = 'tiff_stacks', #The folder with the ometiffs\n",
    "            unstacked_output_folder = 'tiffs', #The name of the folder where tiffs will be extracted\n",
    "            use_panel_files=True): #Use panel files created by BM pipeline for each ROI\n",
    "\n",
    "    global channel_df\n",
    "    global all_data_channels\n",
    "    global roi_data\n",
    "    \n",
    "    # Make output directories if they don't exist\n",
    "    input_folder = Path(input_folder)\n",
    "    output = Path(unstacked_output_folder)\n",
    "    output.mkdir(exist_ok=True)\n",
    "\n",
    "    # Setup a blank dataframe ready to add to\n",
    "    if use_panel_files:\n",
    "        #roi_data = pd.DataFrame(columns=['panel_filename','channel_name','channel_label','filename','folder','fullstack_path'])\n",
    "        roi_data = pd.DataFrame(columns=['channel_name','channel_label'])\n",
    "    # Get a list of all the .tiff files in the input directory\n",
    "    tiff_files = list(input_folder.rglob('*.tiff'))\n",
    "\n",
    "    print('Unpacking ROIs...')\n",
    "    for roi_count,i in enumerate(tqdm(tiff_files)):\n",
    "\n",
    "        image = tp.imread(str(i))    \n",
    "\n",
    "        folder_name = os.path.splitext(os.path.basename(i))[0]\n",
    "\n",
    "        tiff_folder_name = os.path.splitext(os.path.basename(i))[0]    \n",
    "        output_dir = Path(unstacked_output_folder,tiff_folder_name)\n",
    "        output_dir.mkdir(exist_ok=True)        \n",
    "\n",
    "        if use_panel_files:\n",
    "\n",
    "            panel_filename = os.path.splitext(os.path.splitext(os.path.basename(i))[0])[0] + '.csv'\n",
    "            panel_path = join(*i.parts[0:-1])\n",
    "            panel_df = pd.read_csv(join(panel_path, panel_filename))\n",
    "            panel_df['fullstack_path'] = copy(str(i))       \n",
    "            panel_df['panel_filename']=panel_filename\n",
    "            panel_df['folder']=folder_name\n",
    "            roi_data = pd.concat([roi_data, panel_df], sort=True)\n",
    "\n",
    "        for channel_count in range(image.shape[0]):\n",
    "\n",
    "            if use_panel_files:\n",
    "\n",
    "                panel_df['filename']=copy(str(channel_count)).zfill(2)+\"_\"+str(roi_count).zfill(2)+\"_\"+panel_df['channel_name']+\"_\"+panel_df['channel_label'].astype(str)+\".tiff\"\n",
    "                tp.imwrite(join(output_dir, panel_df.loc[channel_count,'filename']), image[channel_count])\n",
    "            else:\n",
    "                file_name=copy(str(channel_count)).zfill(2)+\"_\"+str(roi_count).zfill(2)+\".tiff\"\n",
    "                tp.imwrite(join(output_dir, file_name), image[channel_count])        \n",
    "\n",
    "    if use_panel_files:\n",
    "        roi_data.to_csv('ROI_data.csv')       \n",
    "        all_data_channels = roi_data.dropna().channel_label.unique().tolist()\n",
    "        all_data_channel_names = roi_data.dropna().channel_name.unique().tolist()\n",
    "        channel_df = pd.DataFrame(list(zip(all_data_channel_names,all_data_channels)), columns = ['channel_name', 'channel_label'])\n",
    "        channel_df['channel']=channel_df['channel_name'] + \"_\" + channel_df['channel_label']\n",
    "        channel_df.to_csv('channels_list.csv')\n",
    "\n",
    "        blank_channels = roi_data[roi_data.channel_label.isna()].channel_name.unique()\n",
    "        n = len(blank_channels)\n",
    "        print(f'The following {n} EMPTY channels were detected, and will be NOT be processed... \\n')\n",
    "        print(roi_data[roi_data.channel_label.isna()].channel_name.unique().tolist())\n",
    "\n",
    "        n = len(all_data_channels)\n",
    "        print(f'\\nThe following {n} channels were detected, and will be used if process_all_channels=True in the next step... \\n')\n",
    "        print(channel_df['channel'])\n",
    "\n",
    "### This is for doing QC heatmaps and PCAs to look for outliers\n",
    "\n",
    "def qc_heatmap(directory='tiffs', #The directory to analyse\n",
    "                quantile=0.95,\n",
    "                save=True, \n",
    "                process_all_channels=False,\n",
    "                channels=[],\n",
    "                normalize=None, #Can be max or zscore\n",
    "                figsize=(20,10),\n",
    "                dpi=200,             \n",
    "                save_dir='qc_images',\n",
    "                do_PCA=True,\n",
    "                annotate_PCA=True,\n",
    "                hide_figures=False):\n",
    "    \n",
    "    if not isinstance(channels, list):\n",
    "        channels=[channels]  \n",
    "\n",
    "    if process_all_channels:\n",
    "        channels = channel_df['channel'].tolist()\n",
    "        \n",
    "    # Create folder for saving\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Create lists to save data into\n",
    "    channel_list=[]\n",
    "    roi_list=[]\n",
    "    img_max_list=[]\n",
    "    img_mean_list=[]\n",
    "    img_std_list=[]\n",
    "    img_q_list=[]\n",
    "\n",
    "    \n",
    "    print('Extracting data from images...\\n')\n",
    "    for channel in tqdm(channels):\n",
    "\n",
    "        Img_collect, Img_file_list, img_folders = load_imgs_from_directory(directory, channel, quiet=True)    \n",
    "\n",
    "        for img,img_f in zip(Img_collect,img_folders):\n",
    "            roi = Path(img_f).parts[1]\n",
    "            img_max=np.max(img)\n",
    "            img_mean=np.mean(img)\n",
    "            img_std=np.std(img)\n",
    "            img_q=np.quantile(img,quantile)\n",
    "\n",
    "            channel_list.append(copy(channel))\n",
    "            roi_list.append(copy(roi))\n",
    "            img_max_list.append(copy(img_max))\n",
    "            img_mean_list.append(copy(img_mean))\n",
    "            img_std_list.append(copy(img_std))\n",
    "            img_q_list.append(copy(img_q))\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(list(zip(channel_list, roi_list, img_max_list, img_mean_list, img_std_list, img_q_list)), columns=['channel','ROI','max','mean', 'std','quantile'])\n",
    "\n",
    "    print('Plotting results...\\n')\n",
    "\n",
    "    for i in ['max','mean','quantile', 'std','quantile']:\n",
    "        results_pivot = pd.pivot_table(results_df, index='channel',columns='ROI', values=i)\n",
    "\n",
    "        if normalize=='max':\n",
    "            results_pivot = results_pivot.div(results_pivot.max(axis=1), axis=0)\n",
    "            i = i+'_max_normalised'\n",
    "        elif normalize=='zscore':\n",
    "            results_pivot = results_pivot.apply(zscore, axis=0)\n",
    "            i = i+'_zscore'         \n",
    "\n",
    "        fig, ax = plt.subplots(figsize=figsize, dpi=dpi)\n",
    "        ax = sb.heatmap(results_pivot,xticklabels=True, yticklabels=True)\n",
    "        plt.title(i)\n",
    "\n",
    "        if save:\n",
    "            fig.savefig(join(save_dir,f'{directory}_{i}_heatmap.png'))\n",
    "        \n",
    "        if hide_figures:\n",
    "            plt.close()\n",
    "                        \n",
    "        if do_PCA:\n",
    "            import sklearn\n",
    "            from sklearn.decomposition import PCA\n",
    "\n",
    "            scaled_summary_data = sklearn.preprocessing.StandardScaler().fit_transform(results_pivot.T)\n",
    "\n",
    "            pca = PCA(n_components=2)\n",
    "            embedding = pca.fit_transform(scaled_summary_data)\n",
    "\n",
    "            #Create the graphs\n",
    "            fig, ax = plt.subplots(figsize=(10,10))\n",
    "            ax.scatter(\n",
    "                embedding[:, 0],\n",
    "                embedding[:, 1],\n",
    "                s=15)\n",
    "            \n",
    "            if annotate_PCA:\n",
    "                for loc, txt in zip(embedding,list(range(len(results_pivot.T.index)))):\n",
    "                    ax.annotate(txt, loc)  \n",
    "                pd.DataFrame(results_pivot.T.index).to_csv(join(save_dir,'roi_annotations.csv'))                    \n",
    "            \n",
    "            fig.gca().set_aspect('equal', 'datalim')\n",
    "            ax.set_xlabel('PCA1')\n",
    "            ax.set_ylabel('PCA2')\n",
    "            plt.title(i)\n",
    "            \n",
    "            if save:\n",
    "                plt.savefig(join(save_dir,f'{directory}_{i}_PCA.png'))\n",
    "                \n",
    "            if hide_figures:\n",
    "                plt.close()\n",
    "\n",
    "#### Deep SNF batch function            \n",
    "            \n",
    "def deep_SNF_batch(raw_directory = \"tiffs\", #Input folder\n",
    "                   processed_output_dir = \"processed\", #Output folder\n",
    "                   process_all_channels = False,\n",
    "                   channels = [], \n",
    "                   patch_step_size=60,\n",
    "                   train_epoches = 50, # 50 gets good results in my experience.\n",
    "                    train_initial_lr = 1e-3, # inital learning rate. The default is 1e-3.\n",
    "                    train_batch_size = 128, # training batch size. For a GPU with smaller memory, it can be tuned smaller. The default is 256.\n",
    "                    pixel_mask_percent = 0.2, # percentage of the masked pixels in each patch. The default is 0.2.\n",
    "                    val_set_percent = 0.15, #percentage of validation set. The default is 0.15.\n",
    "                    loss_function = \"I_divergence\", # loss function used. The default is \"I_divergence\".\n",
    "                    loss_name = None, # training and validation losses saved here, either .mat or .npz format. If not defined, the losses will not be saved.\n",
    "                    weights_save_directory = None, # location where 'weights_name' and 'loss_name' saved.\n",
    "                    # If the value is None, the files will be saved in a sub-directory named \"trained_weights\" of  the current file folder.\n",
    "                    is_load_weights = False, # Use the trained model directly. Will not read from saved one.\n",
    "                    lambda_HF = 3e-6,\n",
    "                    n_neighbours = 4, # Larger n enables removing more consecutive hot pixels \n",
    "                    n_iter = 3, # Iteration number for DIMR\n",
    "                    window_size = 3): # HF regularization parameter\n",
    "\n",
    "    # Error catching to make specific channels a list if just one channel given\n",
    "    if not isinstance(channels, list):\n",
    "        channels=[channels]  \n",
    "\n",
    "    # Training settings\n",
    "    row_step=patch_step_size\n",
    "    col_step=patch_step_size \n",
    "\n",
    "    # Create folders\n",
    "    processed_output_dir = Path(processed_output_dir)\n",
    "    processed_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Error catching lists\n",
    "    error_channels=[]\n",
    "    completed_channels=[]\n",
    "\n",
    "    if process_all_channels:\n",
    "        channels = channel_df['channel'].tolist()\n",
    "\n",
    "    n = len(channels)\n",
    "    print(f'\\nPerforming denoising on the following {n} channels... \\n')\n",
    "    print(channels)\n",
    "    \n",
    "        \n",
    "    for channel_name in tqdm(channels):\n",
    "\n",
    "        try:\n",
    "\n",
    "            if 'generated_patches' in globals():\n",
    "                del globals.generated_patches    \n",
    "\n",
    "            if not is_load_weights:\n",
    "                DataGenerator = DeepSNF_DataGenerator(channel_name = channel_name, \n",
    "                                                      n_neighbours = n_neighbours, # Larger n enables removing more consecutive hot pixels \n",
    "                                                      n_iter = n_iter, # Iteration number for DIMR\n",
    "                                                      window_size = window_size, # Slide window size. For IMC images, window_size = 3 is fine.\n",
    "                                                      col_step=col_step,\n",
    "                                                      row_step=row_step)\n",
    "\n",
    "                generated_patches = DataGenerator.generate_patches_from_directory(load_directory = raw_directory)\n",
    "                print('The shape of the generated training set is ' + str(generated_patches.shape) + '.')\n",
    "\n",
    "            weights_name=\"weights_\"+str(channel_name)+\".hdf5\"\n",
    "            \n",
    "            deepsnf = DeepSNF(train_epoches = train_epoches, \n",
    "                              train_learning_rate = train_initial_lr,\n",
    "                              train_batch_size = train_batch_size,\n",
    "                              mask_perc_pix = pixel_mask_percent,\n",
    "                              val_perc = val_set_percent,\n",
    "                              loss_func = loss_function,\n",
    "                              weights_name = weights_name,\n",
    "                              loss_name = loss_name,\n",
    "                              weights_dir = weights_save_directory, \n",
    "                              is_load_weights = is_load_weights,\n",
    "                              lambda_HF = lambda_HF)\n",
    "\n",
    "            \n",
    "            if not is_load_weights:\n",
    "                print('STARTING TRAINING...')\n",
    "                # Train the DeepSNF classifier \n",
    "                train_loss, val_loss = deepsnf.train(generated_patches)\n",
    "            else:\n",
    "                print(f'Using weights file: {weights_name}')\n",
    "\n",
    "            # Load all images\n",
    "            Img_collect, Img_file_list, img_folders = load_imgs_from_directory(raw_directory, channel_name)\n",
    "\n",
    "            # Save resulting images\n",
    "            for i, img_file_name, folder in zip(Img_collect, Img_file_list, img_folders):\n",
    "\n",
    "                #Perform both the hot pixel and shot noise \n",
    "                Img_DIMR_DeepSNF = deepsnf.perform_IMC_Denoise(i, n_neighbours = n_neighbours, n_iter = n_iter, window_size = window_size)\n",
    "\n",
    "                #Gets the ROI folder name from the path\n",
    "                roi_folder_name = Path(folder).parts[1]\n",
    "\n",
    "                #Makes sure the output folder name exists for this ROI\n",
    "                Path(join(processed_output_dir, roi_folder_name)).mkdir(exist_ok=True) \n",
    "\n",
    "                #The output file is named the same as the input file\n",
    "                save_path = join(processed_output_dir, roi_folder_name, img_file_name)      \n",
    "\n",
    "                #Save the denoised file\n",
    "                tp.imsave(save_path,Img_DIMR_DeepSNF.astype('float32'))\n",
    "\n",
    "            completed_channels.append(channel_name)\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error in channel {channel_name}: {Exception}: {e}\")\n",
    "            error_channels.append(f\"{channel_name}: {Exception}: {e}\")\n",
    "\n",
    "    print(\"Successfull with channels:\")\n",
    "    print(completed_channels)\n",
    "    print(\"Channels with errors:\")\n",
    "    print(error_channels)\n",
    "    deep_SNF_batch.completed = completed_channels\n",
    "    deep_SNF_batch.errors = error_channels\n",
    "    \n",
    "\n",
    "# This function copies all the original tiffs into a new folder, then copies over the processed ones, giving a new folder with all the right images that match up with the original ometiffs    \n",
    "    \n",
    "def combine(raw_directory = \"tiffs\",\n",
    "            processed_output_dir = \"processed\",\n",
    "            combined_dir=\"combined\"):\n",
    "    \n",
    "    # Create folders\n",
    "    combined_dir_create = Path(combined_dir)\n",
    "    combined_dir_create.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Copy raw tiffs into new directory\n",
    "    print(f'Copying original files from: {raw_directory}...')\n",
    "    distutils.dir_util.copy_tree(raw_directory, combined_dir)\n",
    "    \n",
    "    # Copy processed tiffs over, hopefully overwriting\n",
    "    print(f'Adding in processed files from: {processed_output_dir}...')\n",
    "    distutils.dir_util.copy_tree(processed_output_dir, combined_dir)\n",
    "    \n",
    "# Side by side comparisson of before and after processing\n",
    "\n",
    "def qc_check_side_by_side(channels=[],\n",
    "                          process_all_channels=False,\n",
    "                            colourmap ='jet',\n",
    "                            dpi=200,\n",
    "                            save=True,\n",
    "                            save_dir='qc_images',\n",
    "                            do_all_channels=True,\n",
    "                            hide_images=True,\n",
    "                            raw_directory='tiffs',\n",
    "                            processed_output_dir='processed',\n",
    "                            quiet=True):\n",
    "    \n",
    "    if not isinstance(channels, list):\n",
    "        channels=[channels]    \n",
    "    \n",
    "    if process_all_channels:\n",
    "        channels = channel_df['channel'].tolist()\n",
    "    \n",
    "    # Create folders\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    if do_all_channels:\n",
    "        channel_list=all_data_channels\n",
    "    else:\n",
    "        channel_list=channels\n",
    "\n",
    "    # Error catching lists\n",
    "    error_channels=[]\n",
    "    completed_channels=[]    \n",
    "\n",
    "    for channel_name in channel_list:\n",
    "\n",
    "        try:\n",
    "\n",
    "            raw_Img_collect, raw_Img_file_list, raw_img_folders = load_imgs_from_directory(raw_directory, channel_name,quiet=quiet)\n",
    "            pro_Img_collect, pro_Img_file_list, pro_img_folders = load_imgs_from_directory(processed_output_dir, channel_name,quiet=quiet)\n",
    "\n",
    "            fig, axs = plt.subplots(len(raw_Img_collect), 2, figsize=(10, 5*len(raw_Img_collect)), dpi=dpi)\n",
    "\n",
    "            count = 0\n",
    "            for r_img,p_img,r_img_name in zip(raw_Img_collect,pro_Img_collect,raw_Img_file_list):\n",
    "                im1= axs.flat[count].imshow(r_img, vmin = 0, vmax = 0.5*np.max(r_img), cmap = colourmap)\n",
    "                divider = make_axes_locatable(axs.flat[count])\n",
    "                cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "                fig.colorbar(im1, cax=cax, orientation='vertical')\n",
    "                axs.flat[count].set_ylabel(str(r_img_name))\n",
    "                count=count+1\n",
    "\n",
    "                im2 = axs.flat[count].imshow(p_img, vmin = 0, vmax = 0.5*np.max(p_img), cmap = colourmap)\n",
    "                divider = make_axes_locatable(axs.flat[count])\n",
    "                cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "                fig.colorbar(im2, cax=cax, orientation='vertical')    \n",
    "                count=count+1 \n",
    "\n",
    "            fig.savefig(join(save_dir, channel_name+'.png'))\n",
    "\n",
    "            if hide_images:\n",
    "                plt.close()\n",
    "\n",
    "            completed_channels.append(channel_name)\n",
    "\n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error in channel {channel_name}: {Exception}: {e}\")\n",
    "            error_channels.append(f\"{channel_name}: {Exception}: {e}\")\n",
    "\n",
    "    print(\"Successfull with channels:\")\n",
    "    print(completed_channels)\n",
    "    print(\"Channels with errors:\")\n",
    "    print(error_channels)\n",
    "    qc_check_side_by_side.completed = completed_channels\n",
    "    qc_check_side_by_side.errors = error_channels        \n",
    "    \n",
    "    \n",
    "# Function to reassemble TIFF stacks - assumes they are in the right order, which by default they should be if 'unstack' is used\n",
    "\n",
    "def reassemble_stacks(restack_input_folder = 'combined',\n",
    "                      restacked_output_folder = 'tiffs_restacked',\n",
    "                      save_panel=True, #Will save a csv file that details each chanel in the new stack\n",
    "                      re_order=None,#Give a list of the file names in their correct order\n",
    "                      ascending_sort_names=True): #If not reordering, this will force a sort of the file names into ascending order\n",
    "    \n",
    "    global file_df\n",
    "        \n",
    "    # Make output directories if they don't exisit\n",
    "    restack_input_folder = Path(restack_input_folder)\n",
    "    output = Path(restacked_output_folder)\n",
    "    output.mkdir(exist_ok=True)\n",
    "\n",
    "    \n",
    "    # Get a list of paths of ROI folder\n",
    "    Img_folders = glob(join(restack_input_folder, \"*\", \"\"))\n",
    "\n",
    "    print('Savings stacks...')\n",
    "    for i in tqdm(Img_folders):\n",
    "\n",
    "        tiff_files = list(Path(i).rglob('*.tiff'))\n",
    "        file_names = [os.path.splitext(os.path.splitext(os.path.basename(tiff_files[x]))[0])[0] for x in range(len(tiff_files))]\n",
    "        file_df = pd.DataFrame(zip(file_names,tiff_files),columns=['File name','Path']).set_index('File name')\n",
    "        \n",
    "        if re_order:\n",
    "            file_df=file_df.reindex(re_order)\n",
    "        elif ascending_sort_names:\n",
    "            file_df=file_df.sort_index(ascending=True)            \n",
    "        \n",
    "        image_stack=[]\n",
    "\n",
    "        for file in file_df['Path']:\n",
    "            im = tp.imread(str(file)).astype('float32')\n",
    "            image_stack.append(im)\n",
    "\n",
    "        image_stack = np.asarray(image_stack)\n",
    "\n",
    "        save_path=join(restacked_output_folder, Path(i).parts[1]+\".tiff\")\n",
    "\n",
    "        tp.imsave(save_path,image_stack.astype('float32'))\n",
    "        \n",
    "        if save_panel:\n",
    "            panel_path =join(restacked_output_folder, Path(i).parts[1]+\".csv\")\n",
    "            file_df.to_csv(panel_path)\n",
    "\n",
    "def gpu_test():\n",
    "    import tensorflow as tf\n",
    "\n",
    "    if tf.test.is_built_with_cuda()==True:\n",
    "        print('GPU accceleration enabled \\n')\n",
    "        print(tf.config.list_physical_devices('GPU'))\n",
    "    else:\n",
    "        print('GPU not found! Check TensorFlow and CUDA setup')\n",
    "\n",
    "def reassemble_stacks2(channels = [], \n",
    "                       restack_input_folder = 'combined',\n",
    "                       restacked_output_folder = 'Deep_Cell',\n",
    "                       process_all_channels=False,\n",
    "                       do_all_channels=True): \n",
    "    \n",
    "    global file_df\n",
    "    \n",
    "    # Make output directories if they don't exisit\n",
    "    restack_input_folder = Path(restack_input_folder)\n",
    "    output = Path(restacked_output_folder)\n",
    "    output.mkdir(exist_ok=True)\n",
    "    \n",
    "    if not isinstance(channels, list):\n",
    "        channels=[channels]    \n",
    "    \n",
    "    if process_all_channels:\n",
    "        channels = channel_df['channel'].tolist()\n",
    "    \n",
    "    if do_all_channels:\n",
    "        channel_list=all_data_channels\n",
    "    else:\n",
    "        channel_list=channels\n",
    "        \n",
    "    n = len(channels)\n",
    "    print(f'\\nStacking the following {n} channels... \\n')\n",
    "    print(channels)\n",
    "\n",
    "    # Get a list of paths of ROI folder\n",
    "    Img_folders = glob(join(restack_input_folder, \"*\", \"\"))\n",
    "        \n",
    "    for channels in channel_list:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            for i in tqdm(Img_folders):\n",
    "\n",
    "                tiff_files = list(Path(i).rglob('*.tiff'))\n",
    "                file_names = [os.path.splitext(os.path.splitext(os.path.basename(tiff_files[x]))[0])[0] for x in range(len(tiff_files))]\n",
    "                file_df = pd.DataFrame(zip(file_names,tiff_files),columns=['File name','Path']).set_index('File name')\n",
    "        \n",
    "            image_stack=[]\n",
    "\n",
    "            for file in file_df['Path']:\n",
    "                im = tp.imread(str(file)).astype('float32')\n",
    "                image_stack.append(im)\n",
    "\n",
    "            image_stack = np.asarray(image_stack)\n",
    "\n",
    "            save_path=join(restacked_output_folder, Path(i).parts[1]+\".tiff\")\n",
    "\n",
    "            tp.imsave(save_path,image_stack.astype('float32'))\n",
    "        \n",
    "        except Exception as e:\n",
    "\n",
    "            print(f\"Error in channel {channel_name}: {Exception}: {e}\")\n",
    "            error_channels.append(f\"{channel_name}: {Exception}: {e}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400839a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the session\n",
    "dill.dump_session('IMC_Denoise_COVID_Malawi.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfaa3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the session\n",
    "dill.load_session('IMC_Denoise_COVID_Malawi.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ba62c",
   "metadata": {},
   "source": [
    "# GPU Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b9aaf3",
   "metadata": {},
   "source": [
    "This should return 'True' and the name of your GPU. If it doesn't, something has gone wrong in the setup of TensorFlow and/or CUDA that allows GPU-acceleration. Without it, the script will run incredibly low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b122dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84eef5c",
   "metadata": {},
   "source": [
    "# 1. Unpack tiff stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa792992",
   "metadata": {},
   "source": [
    "A- input_folder = The folder where the stacked tiff files are. You should be able to just copy and paste the whole .ome.tiff folder that the Bodenmiller pipeline creates after it has extracted the tiff files from the MCD files. This folder also contains the .csv panel files, copy those too! Any panorama files will also be in the same folder, but they won't be used here.\n",
    "B- unstacked_output_folder = Where the 'unstacked' tiff files will be stored. They will be unpacked into a single folder per ROI.\n",
    "C- use_panel_files = If you are using the Bodenmiller pipeline, leave this as True. It will use the .csv panel files for each ROI to properly label the unpacked channels with their metal tags and antigen targets, and will create a file called ROI_data.csv which will store all the information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e845d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unstack_tiffs() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1092c3",
   "metadata": {},
   "source": [
    "By default, 'all_channels' will be all the channels with names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0c886088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Y89_Sma',\n",
       " 'In113_Cd68',\n",
       " 'In115_Cd235ab',\n",
       " 'La139_Pan-cytokeratin',\n",
       " 'Pr141_Cd38',\n",
       " 'Nd142_MHC1',\n",
       " 'Nd143_Vimentin',\n",
       " 'Nd144_CD14',\n",
       " 'Nd145_ICAM1',\n",
       " 'Nd146_CD16',\n",
       " 'Sm147_iNOS',\n",
       " 'Nd148_CD66b',\n",
       " 'Sm149_CD11b',\n",
       " 'Nd150_Cd44',\n",
       " 'Eu151_Cd107a',\n",
       " 'Sm152_Cd45',\n",
       " 'Eu153_Cd31',\n",
       " 'Sm154_CD11c',\n",
       " 'Gd155_Foxp3',\n",
       " 'Gd156_Cd4',\n",
       " 'Gd158_SARS-Cov2',\n",
       " 'Tb159_vWF',\n",
       " 'Gd160_Vista',\n",
       " 'Dy161_Cd20',\n",
       " 'Dy162_Cd8a',\n",
       " 'Dy163_Iba1',\n",
       " 'Dy164_Arginase1',\n",
       " 'Ho165_Fibrinogen',\n",
       " 'Er166_Cd74',\n",
       " 'Er167_GranzymeB',\n",
       " 'Er168_Ki67',\n",
       " 'Tm169_Collagen1',\n",
       " 'Er170_Cd3',\n",
       " 'Yb171_pERK',\n",
       " 'Yb172_ClvdCaspase3',\n",
       " 'Yb173_Cd45RO',\n",
       " 'Yb174_MHC2',\n",
       " 'Lu175_CD206',\n",
       " 'Ir191_DNA1',\n",
       " 'Ir193_DNA3',\n",
       " 'Pt196_Cd163']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_channels=channel_df['channel'].tolist()\n",
    "all_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb2d2c",
   "metadata": {},
   "source": [
    "# 2. QC check on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10858d1",
   "metadata": {},
   "source": [
    "This does some QC checks on the raw data, and isn't strictly necessary for later analyses. By default it won't be displayed, but will be saved to a new'qc_images' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01e2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the function in first box or run this for the default options\n",
    "qc_heatmap(channels=all_channels,\n",
    "          hide_figures=True) #This will not display all the images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c30b50",
   "metadata": {},
   "source": [
    "# 3. Run DeepSNF training and image denoising"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99aa0b1",
   "metadata": {},
   "source": [
    "channels = Specify which channels to process here, e.g. if you only want to process a couple. When you define channels, it will search through the source directories for any images matching that name. Therefore, be careful with channels with overlapping names! Eg. CD4 and CD45!\n",
    "raw_directory = This should be the same as 'unstacked_output_folder' above (by default is) - where the unstacked images were stored, with each ROI being a folder containing all its images.\n",
    "processed_output_dir = The folder where the processed images will be stored. They will be in the same format as above - each ROI its own folder containing all its images.\n",
    "Deep SNF settings\n",
    "These all have accompanying explanations, and can mostly be left alone. Ones you may want to change include...\n",
    "train_batch_size If you are getting 'out of memory' errors you may need to reduce this to work on a GPU (e.g. to 32), or increase if you have a very good GPU setup.\n",
    "patch_step_size This is the frequency (in pixels) at which patches are taken from the dataset for training. If you are getting errors of being out of memory, usually because you have a huge dataset, increase this from its default of 60, to 100-150. Also, if you have a channel with very few cells or sparse taining, you may also want to decrease this a lot, potentially to <50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c220ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deep_SNF_batch(channels=[\"Ir193_DNA3\"],  #This will do all channels, as we defined above, but you can just give a list of channels\n",
    "               patch_step_size=80,\n",
    "               train_batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec0bb81",
   "metadata": {},
   "source": [
    "# 4. Side-by-side comparisson to check performance of denoising¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5c5eba",
   "metadata": {},
   "source": [
    "This will do a side-by-side comparisson for each ROI for before and after denoising, for each channel, then save the image to the 'qc_images' directory. By default, it will look for the raw and processed images in the directories specified above, but you can point to specific directories insteead (raw_directory and processed_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7011b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_check_side_by_side(channels=all_channels, \n",
    "                      dpi=100), #By default ever ROI is done side-by-side, which can make images huge!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d018236",
   "metadata": {},
   "source": [
    "# 5. QC check on processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e041c8",
   "metadata": {},
   "source": [
    "This will create heatmaps and PCAs that can be compared with those generated in step 2 on the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53dc25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_heatmap(directory='processed', normalize='max', channels=all_channels, hide_figures=True)\n",
    "qc_heatmap(directory='processed', normalize='zscore', channels=all_channels, hide_figures=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2d882d",
   "metadata": {},
   "source": [
    "# 6. Combine\n",
    "Create a new folder which will contain all the newly processed tiffs, and the original tiffs which were not processed. **If you do not want to use all the newly processed tiffs, then you will have to do this step manually!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb116585",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedda4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#not working\n",
    "#transfer(channels= \"Er166\", process_all_channels=False, do_all_channels=False,\n",
    "           # raw_directory = \"tiffs\",\n",
    "           # processed_output_dir = \"processed2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8ed9ad",
   "metadata": {},
   "source": [
    "# 7. Reassemble TIFF stacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09326565",
   "metadata": {},
   "source": [
    "At this point, we want to reassemble the invidiual images back into stacks so we can put them back into the Bodenmiller pipeline, replacing the ones originally generated. You may want to keep backups of the unprocessed tiffs!\n",
    "By default, this pipeline will use all the processed image! If you only want to use some of the images, then manually assemble the individual TIFFs in the folders ready to be restacked\n",
    "restack_input_folder = This should be the same as processed output directory above - where the processed images were stored, with each ROI being a folder containing all its images. Default is 'tiffs'.\n",
    "restack_input_folder = Where the processed and now restacked images should be place. Default is 'tiffs_restacked'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262e15eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reassemble_stacks(restack_input_folder = 'combined',\n",
    "                  restacked_output_folder = 'Deep_Cell',                  \n",
    "                  save_panel=True, \n",
    "                  re_order=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0817b36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacking the following 2 channels... \n",
      "\n",
      "['Y89_Sma', 'In113_Cd68']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1476.61it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2154.88it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2014.92it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2174.92it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2230.47it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2018.43it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1772.45it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2141.50it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2136.57it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1928.23it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2193.24it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2101.85it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2137.35it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1794.98it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2004.00it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2105.36it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1781.30it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2193.43it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1920.23it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2101.02it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2015.70it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2226.03it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2105.94it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1795.38it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1879.73it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2034.25it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1958.50it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2112.36it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2067.88it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2068.75it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2139.33it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2158.15it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2119.95it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1642.90it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2139.19it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2058.50it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1970.07it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2095.54it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2234.04it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 2173.44it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n",
      "100%|███████████████████████████████████████| 131/131 [00:00<00:00, 1951.23it/s]\n",
      "/var/folders/0m/4rwt_z816mlcps48xsc34jvc0000gn/T/ipykernel_8123/1548659390.py:559: DeprecationWarning: <tifffile.imsave> is deprecated. Use tifffile.imwrite\n",
      "  tp.imsave(save_path,image_stack.astype('float32'))\n"
     ]
    }
   ],
   "source": [
    "reassemble_stacks2(channels = [\"Y89_Sma\", \"In113_Cd68\"],\n",
    "                   restack_input_folder = 'combined',\n",
    "                   restacked_output_folder = 'Deep_Cell',\n",
    "                   process_all_channels=False,\n",
    "                   do_all_channels=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4720e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (IMC_Denoise)",
   "language": "python",
   "name": "imc_denoise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
